Examples of Hypergraph
========

We briefly introduce the fundamental properties, basic operations, and node classification task on hypergraph with **EasyGraph**.

Basic Properties and Operation of Hypergraph
-------------------------

.. important::

    Each hyperedge in the hypergraph is an unordered set of vertices, which means that ``(0, 1, 2)``, ``(0, 2, 1)``, and ``(2, 1, 0)`` are all the same hyperedge.

>>> import torch
>>> import easygraph as eg
>>> # Create a hypergraph with five nodes and three hyperedges.
>>> hg = eg.Hypergraph(5, [(0, 1, 2), (2, 3), (0, 4)])
>>> # print hyperedges with weights of hypergraph.
>>> hg.e
([(0, 1, 2), (2, 3), (0, 4)], [1.0, 1.0, 1.0])
>>> # print the incidence matrix of the hypergraph
>>> hg.H.to_dense()
tensor([[1., 0., 1.],
        [1., 0., 0.],
        [1., 1., 0.],
        [0., 1., 0.],
        [0., 0., 1.]])
>>> # Draw hypergraph with label of each vertex, color of vertexs and edges are set to blue and yellow respectively, line width of each hyperedges depends on its weight.
>>> hg.draw(v_label = [0,1,2,3,4], v_color = 'b', e_color = 'y', e_line_width = hg.e[1])

.. image:: hypergraph_example1.png



Add hyperedges and you can find the weight of the last hyperedge is 1.0 and 2.0, if you set the merge_op to mean and sum, respectively.

>>> hg = eg.Hypergraph(5, [(0, 1, 2), (2, 3), (2, 3), (0, 4)], merge_op="mean")
>>> hg.e
([(0, 1, 2), (2, 3), (0, 4)], [1.0, 1.0, 1.0])
>>> hg.add_hyperedges([(0, 2, 1), (0, 4)], merge_op="mean")
>>> hg.e
([(0, 1, 2), (2, 3), (0, 4), (2, 4)], [1.0, 2.0, 1.0, 1.0])
>>> hg.add_hyperedges([(2, 4)], merge_op="sum")
>>> hg.e
([(0, 1, 2), (2, 3), (0, 4)], [2.0, 2.0, 2.0])
>>> hg.remove_hyperedges([(2, 3)])
>>> hg.e
([(0, 1, 2), (0, 4), (2, 4)], [1.0, 1.0, 1.0])

.. note::

    If the added hyperedges have duplicate hyperedges, those duplicate hyperedges will be automatically merged with specified merge_op.
    If merge_op = 'sum', the weight is the sum of duplicate hyperedges weights.
    If merge_op = 'mean', the weight is the average of sum of duplicate hyperedges weights.


.. image:: hypergraph_example2.png

Create a hypergraph based on the k-nearest neighbors of the features.

>>> X = torch.tensor([[0.0658, 0.3191, 0.0204, 0.6955],
                      [0.1144, 0.7131, 0.3643, 0.4707],
                      [0.2250, 0.0620, 0.0379, 0.2848],
                      [0.0619, 0.4898, 0.9368, 0.7433],
                      [0.5380, 0.3119, 0.6462, 0.4311]])
>>> hg = eg.Hypergraph.from_feature_kNN(X, k=3)
>>> hg
Hypergraph(num_v=5, num_e=4)
>>> hg.e
([(0, 1, 2), (0, 1, 4), (0, 2, 4), (1, 3, 4)], [1.0, 1.0, 1.0, 1.0])
>>> hg.H.to_dense()
tensor([[1., 1., 1., 0.],
        [1., 1., 0., 1.],
        [1., 0., 1., 0.],
        [0., 0., 0., 1.],
        [0., 1., 1., 1.]])
>>> hg.draw(v_label=list(range(0,10)))

.. image:: hypergraph_example3.png

Construct a hypergraph from a graph.

>>> g = eg.Graph()
>>> g.add_edges([(0, 1), (1, 2), (2, 3), (1, 4)])
>>> hg = eg.Hypergraph.from_graph(g)
>>> hg.e
([(0, 1), (1, 2), (1, 4), (2, 3)], [1.0, 1.0, 1.0, 1.0])
>>> hg.H.to_dense()
tensor([[1., 0., 0., 0.],
        [1., 1., 1., 0.],
        [0., 1., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 1., 0.]])

Train a hypergraph neural network model, HGNN+ on trivago-clicks dataset
-------------------------

We present a specifical node classification task on trivago-clicks dataset with a hypergraph neural network HGNN


**Model:**

HGNN (eg.HGNN): Hypergraph Neural Networks


(Feng, Y., You, H., Zhang, Z., Ji, R., & Gao, Y. (2019). Hypergraph Neural Networks. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01), 3558-3565. ).

**Dataset:**

trivago-clicks (eg.trivago_clicks):
Sets of hotels clicked on in a Web browsing session, where labels are the countries of the accomodation.


**Import Libraries**

.. code-block:: python

    import torch
    import torch.nn as nn
    import numpy as np
    import easygraph as eg
    from sklearn.model_selection import train_test_split
    import matplotlib.pyplot as plt

**Dataset preparation and model definition**

.. code-block:: python

    def preprocess():
    """Preprocess for HGNN model training.

        Preprocess dataset and model

        Returns:
            bool: dataset and model
    """


    # There is no default feature vector for this dataset. Users can generate their own features.
    # Here we use random initialisation to generate 100-dimensional node feature vectors

    trivago_clicks = eg.trivago_clicks()
    node_labels = trivago_clicks["labels"]
    hyperedges = trivago_clicks["edge_list"]
    num_classes = trivago_clicks["num_classes"]
    num_vertices = trivago_clicks["num_vertices"]
    input_feature_dim = 100
    hidden_dim = 64
    num_features = 100

    node_features = {}
    for i in range(len(node_labels)):
        node_features[i] = np.random.randn(num_features)
    '''
    Since there is no default split for this dataset, here we split the test set, validation set, and test set in a 50:25:25 fashion
    '''
    train_nodes, test_nodes = train_test_split(list(range(num_vertices)), test_size=0.25, random_state=42)
    train_nodes, val_nodes = train_test_split(train_nodes, test_size=0.25, random_state=42)
    train_mask = train_nodes
    val_mask = val_nodes
    test_mask = test_nodes

    X = np.array([node_features[node] for node in range(len(node_labels))])
    X = torch.from_numpy(X).float()

    y = np.array([node_labels[node] for node in range(len(node_labels))])
    y = torch.from_numpy(y)

    dataset = {}
    dataset["structure"] = eg.Hypergraph(num_v=len(node_labels), e_list=hyperedges)
    dataset["features"] = X
    dataset["labels"] = y
    dataset["train_mask"] = train_mask
    dataset["val_mask"] = val_mask
    dataset["test_mask"] = test_mask
    dataset["num_classes"] = num_classes

    model = eg.HGNN(in_channels = input_feature_dim, hid_channels = hidden_dim,
                         num_classes = num_classes)

    return dataset, model

**Train, valid, test**

.. code-block:: python

    def train(
        data: dict,
        model: nn.Module,
        optimizer: torch.optim.Optimizer,
        criterion: nn.Module,):

        features, structure = data["features"], data["structure"]
        train_mask, labels = data["train_mask"], data["labels"]
        optimizer.zero_grad()
        outputs = model(features, structure)
        loss = criterion(outputs[train_mask], labels[train_mask])
        loss.backward()
        optimizer.step()
        return loss

    @torch.no_grad()
    def valid(model: nn.Module, data: dict):
        features, structure = data["features"], data["structure"]
        val_mask, labels = data["val_mask"], data["labels"]
        model.eval()
        outputs = model(features, structure).argmax(dim=1)
        correct = (outputs[val_mask] == labels[val_mask]).sum()
        acc = int(correct) / len(val_mask)
        return acc

    @torch.no_grad()
    def test(model: nn.Module, data: dict):
        features, structure = data["features"], data["structure"]
        val_mask, labels = data["test_mask"], data["labels"]
        outputs = model(features, structure).argmax(dim=1)
        correct = (outputs[val_mask] == labels[val_mask]).sum()
        acc = int(correct) / len(val_mask)
        return acc


**Loss visualization**

.. code-block:: python

    def draw_loss_curve(loss1, save_path = "loss_pic.png"):
        plt.clf()
        epochs = range(1, len(loss1) + 1)
        plt.plot(epochs, loss1, 'b', label='EG Training loss')
        plt.title('Training Loss Comparison')
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        if save_path is not None:
            plt.savefig(save_path)
        plt.show()

**Main**

.. code-block:: python

    if __name__ == "__main__":
        dataset, model = preprocess()
        loss_lst = []
        epoch = 10
        lr = 0.01
        loss_fn = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr = lr)
        model.train()
        for i in range(epoch):
            loss = train(data = dataset, model = model, optimizer=optimizer, criterion=loss_fn)
            loss_lst.append(loss.detach().numpy())
            val_acc = valid(model = model, data = dataset)
            print(f"epoch: {i}, valid accuracy : {val_acc}, loss : {loss}")
        print("Training finish!")
        test_acc = test(model = model, data=dataset)
        print("test accuracy:", test_acc)
        draw_loss_curve(loss_lst)

**Output**

.. code-block:: python

        epoch: 0, valid accuracy : 0.05572879681373306, loss : 5.096134662628174
        epoch: 1, valid accuracy : 0.0624903516626015, loss : 4.997525215148926
        epoch: 2, valid accuracy : 0.064589829880515, loss : 4.898364067077637
        epoch: 3, valid accuracy : 0.06554694495044613, loss : 4.788722515106201
        epoch: 4, valid accuracy : 0.06662755873907808, loss : 4.667123794555664
        epoch: 5, valid accuracy : 0.06810954336348761, loss : 4.536109924316406
        epoch: 6, valid accuracy : 0.06971502670659792, loss : 4.4008684158325195
        epoch: 7, valid accuracy : 0.07159838216678502, loss : 4.268074035644531
        epoch: 8, valid accuracy : 0.07274074531476736, loss : 4.144204139709473
        epoch: 9, valid accuracy : 0.07468585013430486, loss : 4.034796714782715
        Training finish!
        test accuracy: 0.07569758017830265


.. image:: loss_pic.png

